{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4SgvqTicFj"
      },
      "source": [
        "#### Install Unsloth on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKW_ANvEibaS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install rouge-score\n",
        "!pip install bert-score\n",
        "! pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n2iCEQC9okE"
      },
      "source": [
        "Complete Evaluation Script with Comparison Visualizations\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Evaluates both base and fine-tuned models and generates comparison charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sI7MKftiWrM",
        "outputId": "1700cf85-07b3-4b7c-aec4-1e317a10ac84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastVisionModel\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# NLP and evaluation imports with safety checks\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "    ROUGE_AVAILABLE = True\n",
        "except:\n",
        "    ROUGE_AVAILABLE = False\n",
        "    print(\"âš  rouge_score not available\")\n",
        "\n",
        "try:\n",
        "    from bert_score import score as bert_score\n",
        "    BERTSCORE_AVAILABLE = True\n",
        "except:\n",
        "    BERTSCORE_AVAILABLE = False\n",
        "    print(\"âš  bert_score not available\")\n",
        "\n",
        "try:\n",
        "    from evaluate import load\n",
        "    METEOR_AVAILABLE = True\n",
        "except:\n",
        "    METEOR_AVAILABLE = False\n",
        "    print(\"âš  evaluate (METEOR) not available\")\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    SENTENCE_TRANSFORMER_AVAILABLE = True\n",
        "except:\n",
        "    SENTENCE_TRANSFORMER_AVAILABLE = False\n",
        "    print(\"âš  sentence_transformers not available\")\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    SPACY_AVAILABLE = True\n",
        "except:\n",
        "    SPACY_AVAILABLE = False\n",
        "    print(\"âš  spacy not available\")\n",
        "\n",
        "try:\n",
        "    from sklearn.metrics import f1_score\n",
        "    SKLEARN_AVAILABLE = True\n",
        "except:\n",
        "    SKLEARN_AVAILABLE = False\n",
        "    print(\"âš  sklearn not available\")\n",
        "\n",
        "class MedicalEvaluator:\n",
        "    \"\"\"Comprehensive medical report evaluator with visualization\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir=\"evaluation_results\"):\n",
        "        self.eval_results = []\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Load spacy if available\n",
        "        self.nlp = None\n",
        "        if SPACY_AVAILABLE:\n",
        "            try:\n",
        "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "            except:\n",
        "                print(\"âš  Could not load spacy model\")\n",
        "\n",
        "        # Medical terminology\n",
        "        self.anatomical_terms = {\n",
        "            \"chest\": [\"lung\", \"heart\", \"chest\", \"thorax\", \"rib\", \"sternum\",\n",
        "                     \"mediastinum\", \"pleura\", \"diaphragm\", \"trachea\", \"bronch\"],\n",
        "            \"abdomen\": [\"liver\", \"kidney\", \"spleen\", \"pancreas\", \"gallbladder\",\n",
        "                       \"stomach\", \"intestine\", \"bowel\", \"periton\"],\n",
        "            \"head\": [\"brain\", \"skull\", \"sinus\", \"orbit\", \"temporal\", \"frontal\",\n",
        "                    \"parietal\", \"occipital\", \"cerebral\"],\n",
        "            \"spine\": [\"vertebra\", \"disc\", \"spinal\", \"cervical\", \"thoracic\",\n",
        "                     \"lumbar\", \"sacral\", \"coccyx\"]\n",
        "        }\n",
        "\n",
        "        self.pathology_terms = {\n",
        "            \"normal\": [\"normal\", \"unremarkable\", \"clear\", \"negative\",\n",
        "                      \"no abnormality\", \"within normal limits\", \"intact\"],\n",
        "            \"abnormal\": [\"abnormal\", \"lesion\", \"mass\", \"nodule\", \"opacity\",\n",
        "                        \"consolidation\", \"effusion\", \"pneumonia\", \"fracture\",\n",
        "                        \"edema\", \"inflammation\", \"infection\", \"tumor\",\n",
        "                        \"hemorrhage\", \"atelectasis\"]\n",
        "        }\n",
        "\n",
        "        self.severity_terms = [\"mild\", \"moderate\", \"severe\", \"marked\",\n",
        "                              \"significant\", \"minimal\", \"extensive\", \"diffuse\"]\n",
        "\n",
        "        self.negation_phrases = [\n",
        "            \"no evidence of\", \"no definite signs of\", \"without evidence of\",\n",
        "            \"no sign of\", \"not seen\", \"negative for\", \"absence of\",\n",
        "            \"unremarkable for\", \"free of\", \"no demonstration of\",\n",
        "            \"there is no\", \"cannot be identified\", \"not identified\",\n",
        "            \"rule out\", \"r/o\", \"unlikely\"\n",
        "        ]\n",
        "\n",
        "        self.location_terms = [\"left\", \"right\", \"bilateral\", \"upper\", \"lower\",\n",
        "                              \"anterior\", \"posterior\", \"medial\", \"lateral\",\n",
        "                              \"proximal\", \"distal\", \"central\", \"peripheral\"]\n",
        "\n",
        "    def extract_medical_entities(self, text):\n",
        "        \"\"\"Extract and classify medical entities from text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        entities = {\n",
        "            \"anatomical\": [],\n",
        "            \"pathological\": [],\n",
        "            \"severity\": [],\n",
        "            \"negations\": [],\n",
        "            \"locations\": []\n",
        "        }\n",
        "\n",
        "        # Extract anatomical structures\n",
        "        for region, terms in self.anatomical_terms.items():\n",
        "            for term in terms:\n",
        "                if term in text_lower:\n",
        "                    entities[\"anatomical\"].append(term)\n",
        "\n",
        "        # Extract pathological findings\n",
        "        for category, terms in self.pathology_terms.items():\n",
        "            for term in terms:\n",
        "                if term in text_lower:\n",
        "                    entities[\"pathological\"].append((term, category))\n",
        "\n",
        "        # Extract severity indicators\n",
        "        for term in self.severity_terms:\n",
        "            if term in text_lower:\n",
        "                entities[\"severity\"].append(term)\n",
        "\n",
        "        # Extract negations\n",
        "        for phrase in self.negation_phrases:\n",
        "            if phrase in text_lower:\n",
        "                entities[\"negations\"].append(phrase)\n",
        "\n",
        "        # Extract locations\n",
        "        for term in self.location_terms:\n",
        "            if term in text_lower:\n",
        "                entities[\"locations\"].append(term)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def compute_medical_metrics(self, preds, refs):\n",
        "        \"\"\"Compute medical-specific evaluation metrics\"\"\"\n",
        "        medical_scores = {\n",
        "            \"anatomical_f1\": 0.0,\n",
        "            \"pathology_classification_f1\": 0.0,\n",
        "            \"negation_handling_accuracy\": 0.0,\n",
        "            \"severity_matching_accuracy\": 0.0,\n",
        "            \"location_accuracy\": 0.0,\n",
        "            \"medical_terminology_coverage\": 0.0,\n",
        "            \"report_completeness\": 0.0\n",
        "        }\n",
        "\n",
        "        pred_entities = [self.extract_medical_entities(p) for p in preds]\n",
        "        ref_entities = [self.extract_medical_entities(r) for r in refs]\n",
        "\n",
        "        # 1. Anatomical Structure F1\n",
        "        anatomy_f1s = []\n",
        "        for pred_ent, ref_ent in zip(pred_entities, ref_entities):\n",
        "            pred_anat = set(pred_ent[\"anatomical\"])\n",
        "            ref_anat = set(ref_ent[\"anatomical\"])\n",
        "\n",
        "            if len(ref_anat) == 0 and len(pred_anat) == 0:\n",
        "                f1 = 1.0\n",
        "            elif len(ref_anat) == 0 or len(pred_anat) == 0:\n",
        "                f1 = 0.0\n",
        "            else:\n",
        "                intersection = len(pred_anat & ref_anat)\n",
        "                precision = intersection / len(pred_anat) if len(pred_anat) > 0 else 0\n",
        "                recall = intersection / len(ref_anat) if len(ref_anat) > 0 else 0\n",
        "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            anatomy_f1s.append(f1)\n",
        "\n",
        "        medical_scores[\"anatomical_f1\"] = np.mean(anatomy_f1s)\n",
        "\n",
        "        # 2. Pathology Classification\n",
        "        if SKLEARN_AVAILABLE:\n",
        "            pred_pathology = []\n",
        "            ref_pathology = []\n",
        "\n",
        "            for pe, re in zip(pred_entities, ref_entities):\n",
        "                pred_abnormal = any(cat == \"abnormal\" for _, cat in pe[\"pathological\"])\n",
        "                ref_abnormal = any(cat == \"abnormal\" for _, cat in re[\"pathological\"])\n",
        "                pred_pathology.append(1 if pred_abnormal else 0)\n",
        "                ref_pathology.append(1 if ref_abnormal else 0)\n",
        "\n",
        "            if len(set(ref_pathology)) > 1:\n",
        "                medical_scores[\"pathology_classification_f1\"] = f1_score(\n",
        "                    ref_pathology, pred_pathology, average='weighted'\n",
        "                )\n",
        "\n",
        "        # 3. Negation Handling\n",
        "        pred_negations = [len(pe[\"negations\"]) > 0 for pe in pred_entities]\n",
        "        ref_negations = [len(re[\"negations\"]) > 0 for re in ref_entities]\n",
        "        negation_matches = sum(1 for p, r in zip(pred_negations, ref_negations) if p == r)\n",
        "        medical_scores[\"negation_handling_accuracy\"] = negation_matches / len(preds) if len(preds) > 0 else 0\n",
        "\n",
        "        # 4. Severity Matching\n",
        "        severity_matches = 0\n",
        "        for pe, re in zip(pred_entities, ref_entities):\n",
        "            pred_sev = set(pe[\"severity\"])\n",
        "            ref_sev = set(re[\"severity\"])\n",
        "            if len(ref_sev) == 0:\n",
        "                if len(pred_sev) == 0:\n",
        "                    severity_matches += 1\n",
        "            else:\n",
        "                if len(pred_sev & ref_sev) > 0:\n",
        "                    severity_matches += 1\n",
        "        medical_scores[\"severity_matching_accuracy\"] = severity_matches / len(preds) if len(preds) > 0 else 0\n",
        "\n",
        "        # 5. Location Accuracy\n",
        "        location_matches = 0\n",
        "        for pe, re in zip(pred_entities, ref_entities):\n",
        "            pred_loc = set(pe[\"locations\"])\n",
        "            ref_loc = set(re[\"locations\"])\n",
        "            if len(ref_loc) == 0:\n",
        "                if len(pred_loc) == 0:\n",
        "                    location_matches += 1\n",
        "            else:\n",
        "                if len(pred_loc & ref_loc) > 0:\n",
        "                    location_matches += 1\n",
        "        medical_scores[\"location_accuracy\"] = location_matches / len(preds) if len(preds) > 0 else 0\n",
        "\n",
        "        # 6. Medical Terminology Coverage\n",
        "        all_medical_terms = set()\n",
        "        for terms_dict in self.anatomical_terms.values():\n",
        "            all_medical_terms.update(terms_dict)\n",
        "        for terms_list in self.pathology_terms.values():\n",
        "            all_medical_terms.update(terms_list)\n",
        "        all_medical_terms.update(self.severity_terms)\n",
        "        all_medical_terms.update(self.location_terms)\n",
        "\n",
        "        terminology_scores = []\n",
        "        for pred, ref in zip(preds, refs):\n",
        "            pred_terms = set(term for term in all_medical_terms if term in pred.lower())\n",
        "            ref_terms = set(term for term in all_medical_terms if term in ref.lower())\n",
        "\n",
        "            if len(ref_terms) == 0:\n",
        "                score = 1.0 if len(pred_terms) == 0 else 0.5\n",
        "            else:\n",
        "                overlap = len(pred_terms & ref_terms)\n",
        "                score = overlap / len(ref_terms)\n",
        "            terminology_scores.append(score)\n",
        "\n",
        "        medical_scores[\"medical_terminology_coverage\"] = np.mean(terminology_scores)\n",
        "\n",
        "        # 7. Report Completeness\n",
        "        completeness_scores = []\n",
        "        for pred_ent, ref_ent in zip(pred_entities, ref_entities):\n",
        "            score = 0\n",
        "            total = 0\n",
        "\n",
        "            if ref_ent[\"anatomical\"]:\n",
        "                total += 1\n",
        "                if pred_ent[\"anatomical\"]:\n",
        "                    score += len(set(pred_ent[\"anatomical\"]) & set(ref_ent[\"anatomical\"])) / len(set(ref_ent[\"anatomical\"]))\n",
        "\n",
        "            if ref_ent[\"pathological\"]:\n",
        "                total += 1\n",
        "                ref_path = set(p[0] for p in ref_ent[\"pathological\"])\n",
        "                pred_path = set(p[0] for p in pred_ent[\"pathological\"])\n",
        "                if pred_path:\n",
        "                    score += len(pred_path & ref_path) / len(ref_path)\n",
        "\n",
        "            if total > 0:\n",
        "                completeness_scores.append(score / total)\n",
        "            else:\n",
        "                completeness_scores.append(1.0)\n",
        "\n",
        "        medical_scores[\"report_completeness\"] = np.mean(completeness_scores)\n",
        "\n",
        "        return medical_scores\n",
        "\n",
        "    def compute_comprehensive_metrics(self, preds, refs):\n",
        "        \"\"\"Compute both standard and medical-specific metrics\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Standard NLP metrics\n",
        "        if ROUGE_AVAILABLE:\n",
        "            rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "            rouge_scores = [rouge.score(ref, pred) for pred, ref in zip(preds, refs)]\n",
        "            metrics['rouge1_f1'] = np.mean([s['rouge1'].fmeasure for s in rouge_scores])\n",
        "            metrics['rouge2_f1'] = np.mean([s['rouge2'].fmeasure for s in rouge_scores])\n",
        "            metrics['rougeL_f1'] = np.mean([s['rougeL'].fmeasure for s in rouge_scores])\n",
        "\n",
        "        # BERTScore\n",
        "        if BERTSCORE_AVAILABLE:\n",
        "            try:\n",
        "                P, R, F1 = bert_score(preds, refs, lang='en', verbose=False)\n",
        "                metrics['bertscore_f1'] = F1.mean().item()\n",
        "            except:\n",
        "                metrics['bertscore_f1'] = 0.0\n",
        "\n",
        "        # METEOR\n",
        "        if METEOR_AVAILABLE:\n",
        "            try:\n",
        "                meteor = load('meteor')\n",
        "                metrics['meteor_score'] = meteor.compute(predictions=preds, references=refs)['meteor']\n",
        "            except:\n",
        "                metrics['meteor_score'] = 0.0\n",
        "\n",
        "        # Semantic Similarity\n",
        "        if SENTENCE_TRANSFORMER_AVAILABLE:\n",
        "            try:\n",
        "                embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "                pred_emb = embedder.encode(preds)\n",
        "                ref_emb = embedder.encode(refs)\n",
        "                sem_sim = [util.cos_sim(p, r).item() for p, r in zip(pred_emb, ref_emb)]\n",
        "                metrics['semantic_similarity'] = np.mean(sem_sim)\n",
        "            except:\n",
        "                metrics['semantic_similarity'] = 0.0\n",
        "\n",
        "        # Medical-specific metrics\n",
        "        medical_metrics = self.compute_medical_metrics(preds, refs)\n",
        "        metrics.update(medical_metrics)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_model(self, model, tokenizer, test_samples, model_name, args, num_samples=None):\n",
        "        \"\"\"Evaluate model on test samples\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Evaluating Model: {model_name}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Prepare model for inference\n",
        "        FastVisionModel.for_inference(model)\n",
        "\n",
        "        preds, refs = [], []\n",
        "        samples_to_eval = test_samples if num_samples is None else test_samples[:num_samples]\n",
        "\n",
        "        # Generate predictions\n",
        "        for i, sample in enumerate(samples_to_eval):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processing {i}/{len(samples_to_eval)}...\")\n",
        "\n",
        "            try:\n",
        "                # Extract images and reference\n",
        "                images = [c[\"image\"] for c in sample[\"messages\"][0][\"content\"] if c[\"type\"] == \"image\"]\n",
        "                instruction = [c[\"text\"] for c in sample[\"messages\"][0][\"content\"] if c[\"type\"] == \"text\"][0]\n",
        "                ref = sample[\"messages\"][1][\"content\"][0][\"text\"].strip()\n",
        "                refs.append(ref)\n",
        "\n",
        "                # Prepare input\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": instruction}\n",
        "                    ]}\n",
        "                ]\n",
        "\n",
        "                text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "                inputs = tokenizer(images[0], text, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "                # Generate prediction\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=256,\n",
        "                        temperature=args.get(\"temperature\", 1.5),\n",
        "                        min_p=0.1,\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                generated = tokenizer.decode(\n",
        "                    outputs[0][inputs.input_ids.shape[1]:],\n",
        "                    skip_special_tokens=True\n",
        "                ).strip()\n",
        "\n",
        "                preds.append(generated)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Error on sample {i}: {e}\")\n",
        "                preds.append(\"\")\n",
        "\n",
        "        # Compute metrics\n",
        "        print(f\"\\nComputing evaluation metrics...\")\n",
        "        metrics = self.compute_comprehensive_metrics(preds, refs)\n",
        "\n",
        "        # Add metadata\n",
        "        metrics.update({\n",
        "            \"model_name\": model_name,\n",
        "            \"test_samples_count\": len(samples_to_eval),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        })\n",
        "\n",
        "        # Save results\n",
        "        self.eval_results.append(metrics)\n",
        "        self.save_results(f\"{model_name}_evaluation.json\")\n",
        "\n",
        "        # Print summary\n",
        "        self.print_summary(metrics)\n",
        "\n",
        "        # Cleanup\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_comparison(self, base_metrics, finetuned_metrics, exp_name=\"comparison\"):\n",
        "        \"\"\"Generate comparison visualizations between base and fine-tuned models\"\"\"\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        fig.suptitle(f'Model Comparison: Base vs Fine-tuned ({exp_name})',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Standard NLP Metrics Comparison\n",
        "        ax1 = axes[0, 0]\n",
        "        nlp_metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1', 'bertscore_f1',\n",
        "                      'meteor_score', 'semantic_similarity']\n",
        "        nlp_labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore', 'METEOR', 'Sem-Sim']\n",
        "\n",
        "        base_nlp = [base_metrics.get(m, 0) for m in nlp_metrics]\n",
        "        ft_nlp = [finetuned_metrics.get(m, 0) for m in nlp_metrics]\n",
        "\n",
        "        x = np.arange(len(nlp_labels))\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, base_nlp, width, label='Base Model', color='#FF6B6B', alpha=0.8)\n",
        "        ax1.bar(x + width/2, ft_nlp, width, label='Fine-tuned', color='#4ECDC4', alpha=0.8)\n",
        "        ax1.set_ylabel('Score', fontsize=11)\n",
        "        ax1.set_title('Standard NLP Metrics', fontsize=12, fontweight='bold')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(nlp_labels, rotation=45, ha='right')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3, axis='y')\n",
        "        ax1.set_ylim([0, 1])\n",
        "\n",
        "        # 2. Medical-Specific Metrics Comparison\n",
        "        ax2 = axes[0, 1]\n",
        "        medical_metrics = ['anatomical_f1', 'pathology_classification_f1',\n",
        "                          'negation_handling_accuracy', 'severity_matching_accuracy',\n",
        "                          'location_accuracy', 'medical_terminology_coverage',\n",
        "                          'report_completeness']\n",
        "        medical_labels = ['Anatomical', 'Pathology', 'Negation', 'Severity',\n",
        "                         'Location', 'Terminology', 'Completeness']\n",
        "\n",
        "        base_med = [base_metrics.get(m, 0) for m in medical_metrics]\n",
        "        ft_med = [finetuned_metrics.get(m, 0) for m in medical_metrics]\n",
        "\n",
        "        x = np.arange(len(medical_labels))\n",
        "\n",
        "        ax2.bar(x - width/2, base_med, width, label='Base Model', color='#FF6B6B', alpha=0.8)\n",
        "        ax2.bar(x + width/2, ft_med, width, label='Fine-tuned', color='#4ECDC4', alpha=0.8)\n",
        "        ax2.set_ylabel('Score', fontsize=11)\n",
        "        ax2.set_title('Medical-Specific Metrics', fontsize=12, fontweight='bold')\n",
        "        ax2.set_xticks(x)\n",
        "        ax2.set_xticklabels(medical_labels, rotation=45, ha='right')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3, axis='y')\n",
        "        ax2.set_ylim([0, 1])\n",
        "\n",
        "        # 3. Improvement Percentage\n",
        "        ax3 = axes[1, 0]\n",
        "        all_metrics = nlp_metrics + medical_metrics\n",
        "        all_labels = nlp_labels + medical_labels\n",
        "\n",
        "        improvements = []\n",
        "        for metric in all_metrics:\n",
        "            base_val = base_metrics.get(metric, 0)\n",
        "            ft_val = finetuned_metrics.get(metric, 0)\n",
        "            if base_val > 0:\n",
        "                improvement = ((ft_val - base_val) / base_val) * 100\n",
        "            else:\n",
        "                improvement = 0\n",
        "            improvements.append(improvement)\n",
        "\n",
        "        colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
        "        ax3.barh(all_labels, improvements, color=colors, alpha=0.7)\n",
        "        ax3.set_xlabel('Improvement (%)', fontsize=11)\n",
        "        ax3.set_title('Relative Improvement Over Base Model', fontsize=12, fontweight='bold')\n",
        "        ax3.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "        ax3.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "        # 4. Radar Chart for Overall Comparison\n",
        "        ax4 = axes[1, 1]\n",
        "\n",
        "        # Select key metrics for radar\n",
        "        radar_metrics = ['rouge1_f1', 'bertscore_f1', 'anatomical_f1',\n",
        "                        'pathology_classification_f1', 'report_completeness']\n",
        "        radar_labels = ['ROUGE-1', 'BERTScore', 'Anatomical', 'Pathology', 'Completeness']\n",
        "\n",
        "        base_radar = [base_metrics.get(m, 0) for m in radar_metrics]\n",
        "        ft_radar = [finetuned_metrics.get(m, 0) for m in radar_metrics]\n",
        "\n",
        "        angles = np.linspace(0, 2 * np.pi, len(radar_labels), endpoint=False).tolist()\n",
        "        base_radar += base_radar[:1]\n",
        "        ft_radar += ft_radar[:1]\n",
        "        angles += angles[:1]\n",
        "\n",
        "        ax4 = plt.subplot(2, 2, 4, projection='polar')\n",
        "        ax4.plot(angles, base_radar, 'o-', linewidth=2, label='Base Model', color='#FF6B6B')\n",
        "        ax4.fill(angles, base_radar, alpha=0.25, color='#FF6B6B')\n",
        "        ax4.plot(angles, ft_radar, 'o-', linewidth=2, label='Fine-tuned', color='#4ECDC4')\n",
        "        ax4.fill(angles, ft_radar, alpha=0.25, color='#4ECDC4')\n",
        "        ax4.set_xticks(angles[:-1])\n",
        "        ax4.set_xticklabels(radar_labels)\n",
        "        ax4.set_ylim(0, 1)\n",
        "        ax4.set_title('Overall Performance Comparison', fontsize=12, fontweight='bold', pad=20)\n",
        "        ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "        ax4.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        output_path = self.output_dir / f\"{exp_name}_model_comparison.png\"\n",
        "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nâœ“ Comparison plots saved to: {output_path}\")\n",
        "        plt.show()\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def save_results(self, filename):\n",
        "        \"\"\"Save evaluation results\"\"\"\n",
        "        filepath = self.output_dir / filename\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(self.eval_results, f, indent=2)\n",
        "        print(f\"\\nâœ“ Results saved to {filepath}\")\n",
        "\n",
        "    def print_summary(self, metrics):\n",
        "        \"\"\"Print evaluation summary\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Model: {metrics['model_name']}\")\n",
        "        print(f\"Samples: {metrics['test_samples_count']}\")\n",
        "\n",
        "        print(f\"\\nStandard NLP Metrics:\")\n",
        "        print(f\"  ROUGE-1 F1:       {metrics.get('rouge1_f1', 0):.4f}\")\n",
        "        print(f\"  ROUGE-2 F1:       {metrics.get('rouge2_f1', 0):.4f}\")\n",
        "        print(f\"  ROUGE-L F1:       {metrics.get('rougeL_f1', 0):.4f}\")\n",
        "        print(f\"  BERTScore F1:     {metrics.get('bertscore_f1', 0):.4f}\")\n",
        "        print(f\"  METEOR:           {metrics.get('meteor_score', 0):.4f}\")\n",
        "        print(f\"  Semantic Sim:     {metrics.get('semantic_similarity', 0):.4f}\")\n",
        "\n",
        "        print(f\"\\nMedical-Specific Metrics:\")\n",
        "        print(f\"  Anatomical F1:    {metrics.get('anatomical_f1', 0):.4f}\")\n",
        "        print(f\"  Pathology F1:     {metrics.get('pathology_classification_f1', 0):.4f}\")\n",
        "        print(f\"  Negation Acc:     {metrics.get('negation_handling_accuracy', 0):.4f}\")\n",
        "        print(f\"  Severity Acc:     {metrics.get('severity_matching_accuracy', 0):.4f}\")\n",
        "        print(f\"  Location Acc:     {metrics.get('location_accuracy', 0):.4f}\")\n",
        "        print(f\"  Terminology Cov:  {metrics.get('medical_terminology_coverage', 0):.4f}\")\n",
        "        print(f\"  Completeness:     {metrics.get('report_completeness', 0):.4f}\")\n",
        "\n",
        "        print(f\"{'='*60}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hIvBqoD4i8um",
        "outputId": "3023d765-40c5-468f-80e5-b60b1aeca3d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40c6453f311d411392c68bcf699ed874",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/567 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2763917ea77c4a5f98f7fff5c7ff6635",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00004.parquet:   0%|          | 0.00/191M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96c5ba53bf734aeb88aad1486e96d506",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00004.parquet:   0%|          | 0.00/188M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e594bb28df1d44da833639a0eceafed5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00002-of-00004.parquet:   0%|          | 0.00/191M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "857101aab6c1407e9884684f5af2be4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00003-of-00004.parquet:   0%|          | 0.00/191M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e69d6ebcd09744eb95d323150686c5f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/val-00000-of-00001.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c928485d6864cb4a00f2b7e94e1d434",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/215M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb5f39f254d94472a47cb766196fbf11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/2069 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3057384cbbe249f0a929031ca056228a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating val split:   0%|          | 0/296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "706378f9f161402b8abc15083a829259",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/590 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: Evaluating BASE MODEL\n",
            "============================================================\n",
            "==((====))==  Unsloth 2025.10.1: Fast Mllama patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e61d4bebebda441cb35b432ebc518d67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a85e903d4fc4f63a7227a3b81c415a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0f314f21f8e4b008c4edf2217abb37c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcbfccd2fa7a45e9bdd209795a0efd07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fed3e94df14f445ab22b09a1bb80a74c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f984f793acad44e19830301a1c3e2820",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8466bf5964b740f88ffc1d50566e31c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae546605353e4a80b830203aeb8a932d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80ff19c9da094125992521d91e78d819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc48d8a6b81f4d76bf1eaa1d2a02e79b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "055927fe77604153b17981700898b98a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Evaluating Model: base_model\n",
            "============================================================\n",
            "\n",
            "Processing 0/590...\n",
            "Processing 10/590...\n",
            "Processing 20/590...\n",
            "Processing 30/590...\n",
            "Processing 40/590...\n",
            "Processing 50/590...\n",
            "Processing 60/590...\n",
            "Processing 70/590...\n",
            "Processing 80/590...\n",
            "Processing 90/590...\n",
            "Processing 100/590...\n",
            "Processing 110/590...\n",
            "Processing 120/590...\n",
            "Processing 130/590...\n",
            "Processing 140/590...\n",
            "Processing 150/590...\n",
            "Processing 160/590...\n",
            "Processing 170/590...\n",
            "Processing 180/590...\n",
            "Processing 190/590...\n",
            "Processing 200/590...\n",
            "Processing 210/590...\n",
            "Processing 220/590...\n",
            "Processing 230/590...\n",
            "Processing 240/590...\n",
            "Processing 250/590...\n",
            "Processing 260/590...\n",
            "Processing 270/590...\n",
            "Processing 280/590...\n",
            "Processing 290/590...\n",
            "Processing 300/590...\n",
            "Processing 310/590...\n",
            "Processing 320/590...\n",
            "Processing 330/590...\n",
            "Processing 340/590...\n",
            "Processing 350/590...\n"
          ]
        }
      ],
      "source": [
        "run_full_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yj_mQ1gkT0a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
